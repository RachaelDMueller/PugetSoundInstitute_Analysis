{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b378e2-e65d-4e03-b48e-1cea1e4a909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import numpy \n",
    "import pandas\n",
    "import pathlib\n",
    "import time\n",
    "from io import StringIO\n",
    "from datetime import date\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587ebafb-8803-441f-b70f-55677944b270",
   "metadata": {},
   "source": [
    "# Create dataframe with nitrogen loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8aeb7e-0c59-4d27-a307-00fe430ccf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Table1_loadings(case):\n",
    "\n",
    "    # open yaml configuration file created by notebook `SSM_config_{case}.ipynb` \n",
    "    with open(f'../../etc/SSM_config_{case}.yaml', 'r') as file:\n",
    "        ssm = yaml.safe_load(file)\n",
    "    # load list of directory paths for SSM nutrient loading .dat files\n",
    "    runs = [*ssm['paths']['nutrient_loading_inputs']]\n",
    "    # print out file list\n",
    "    for run in runs:\n",
    "        print(ssm['paths']['nutrient_loading_inputs'][run])\n",
    "    # establish directory path for ssm_pnt_wq_station_info.xlsx\n",
    "    in_dir = pathlib.Path('/mmfs1/gscratch/ssmc/USRS/PSI/Rachael/projects/KingCounty/SalishSeaModel-spreadsheets')\n",
    "\n",
    "    ### START: move to util script ###\n",
    "    \n",
    "    # Adaptation of Ben's method for processing output    \n",
    "    inputs = {}\n",
    "    for run in runs:\n",
    "        with open(ssm['paths']['nutrient_loading_inputs'][run]) as f:\n",
    "            # The parsing logic here is is derived from the linkage instructions for the\n",
    "            # model and direct examination of the source code\n",
    "\n",
    "            # The first line does not contain important information and is treated only like\n",
    "            # a filetype magic\n",
    "            next(f)\n",
    "\n",
    "            # The total number of discharge nodes\n",
    "            num_qs = int(next(f))\n",
    "            # All the node numbers with discharges\n",
    "            #nodes = np.loadtxt([next(f) for l in range(num_qs)], comments='!', dtype=int)\n",
    "            node_raw = StringIO('\\n'.join([next(f) for l in range(num_qs)]))\n",
    "            node_df = pandas.read_csv(node_raw, sep='\\s+!\\s+', names=('Node','Comment'),\n",
    "                                  dtype={'Node':numpy.int64,'Comment':object}, engine='python')\n",
    "            node_df.set_index('Node', inplace=True)\n",
    "            nodes = node_df.index.to_numpy()\n",
    "            # Depth distribution fractions into each node. Skipping the first (node count) column\n",
    "            vqdist = numpy.loadtxt([next(f) for l in range(num_qs)])[:,1:]\n",
    "\n",
    "            num_times = int(next(f))\n",
    "\n",
    "            # Initialize storage arrays\n",
    "            times = numpy.zeros(num_times)\n",
    "            qs = numpy.zeros((num_times, num_qs))\n",
    "            # State variables in the order they are present in the file. These are also going\n",
    "            # to be the NetCDF variable names\n",
    "            statevars = ('discharge', 'temp', 'salt', 'tss',  'alg1', 'alg2', 'alg3', 'zoo1',\n",
    "                                      'zoo2', 'ldoc', 'rdoc', 'lpoc', 'rpoc', 'nh4',  'no32',\n",
    "                                      'urea', 'ldon', 'rdon', 'lpon', 'rpon', 'po4',  'ldop',\n",
    "                                      'rdop', 'lpop', 'rpop', 'pip',  'cod',  'doxg', 'psi',\n",
    "                                      'dsi',  'alg1p','alg2p','alg3p','dic',  'talk')\n",
    "            inputs[run] = {}\n",
    "            for v in statevars:\n",
    "                inputs[run][v] = numpy.zeros((num_times, num_qs))\n",
    "\n",
    "            for t in range(num_times):\n",
    "                times[t] = float(next(f))\n",
    "                for v in statevars:\n",
    "                    inputs[run][v][t,:] = numpy.loadtxt([next(f)])\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Create dictionaries of loading and discharge information by run\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    loading={}\n",
    "    total_flow={}\n",
    "    total_nitrogen={}\n",
    "    #total_annual_nitrogen={}\n",
    "    for run in runs:\n",
    "        print(run)\n",
    "        loading[run] = (inputs[run]['discharge'] * (inputs[run]['nh4'] + inputs[run]['no32']) * 24 * 3600)/1000 #m3/s*mg/l -> kg/day\n",
    "        total_flow[run] = (inputs[run]['discharge'] * 24 * 3600).sum(axis=0) # annual discharge m3/year\n",
    "        total_nitrogen[run]=loading[run][1:].sum(axis=0) # annual loading over the last 365 days of model run(g/year)\n",
    "        #total_annual_nitrogen[run] = total_nitrogen[run].sum() # g/year -> kg/year\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # import source location names from excel\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # information in excel spreadsheet: type (\" River\" or \" Point Source\"), node_id, layer_distribute, basin, country, NH4[kg/year], NO3NO2[kg/year], N-load\n",
    "    source_locations = pandas.read_excel(in_dir/'ssm_pnt_wq_station_info.xlsx',index_col='Unnamed: 0')\n",
    "    river_locations = source_locations[source_locations['type']==' River']\n",
    "    wwtp_locations = source_locations[source_locations['type']==' Point Source']\n",
    "    # make lists of names for: \n",
    "    all_rivers_list = river_locations.index.to_list() # river input locations\n",
    "    all_wwtps_list = wwtp_locations.index.to_list()   # WWTP input locations\n",
    "    all_locations_list = source_locations.index.to_list()  # both river and WWTP input locations\n",
    "    all_locations_type_list = source_locations['type'].to_list() # type of input (\" River\" or \" Point Source\") to eliminate duplicate names\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Create dataframes of information using names from excel as indices\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # NOTE: Some names are redundant for rivers and WWTP so we need to use both name and type (\" River\" vs. \" Point Source\")\n",
    "    total_flow_df  = pandas.DataFrame(\n",
    "        total_flow, \n",
    "        index=all_locations_list,\n",
    "        columns=ssm['run_information']['run_tag'][case] # Assign report names to columns\n",
    "    )\n",
    "    total_loading_df = pandas.DataFrame(\n",
    "        total_nitrogen, \n",
    "        index=all_locations_list,\n",
    "        columns=ssm['run_information']['run_tag'][case] # Assign report names to columns\n",
    "    )\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Use both name of source and type to create lists of wwtp and river sources\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Create boolean arrays with True for only WWTP and River sources used in this cases \n",
    "    case_wwtp = numpy.asarray(total_loading_df.index.isin(ssm['WWTP_names']) & (source_locations['type']==' Point Source'))\n",
    "    case_river = numpy.asarray(total_loading_df.index.isin(ssm['river_names']) & (source_locations['type']==' River'))\n",
    "    # Create boolean arrays with True for all WWTP and River sources used in this study\n",
    "    all_wwtp= numpy.asarray(total_loading_df.index.isin(all_wwtps_list) & (source_locations['type']==' Point Source'))\n",
    "    all_river = numpy.asarray(total_loading_df.index.isin(all_rivers_list) & (source_locations['type']==' River'))\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Calculate the total nitrogen loading for rivers and wwtps\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # modified in a particular case\n",
    "    total_scenario_wwtp_nitrogen_df = total_loading_df[case_wwtp]\n",
    "    total_scenario_river_nitrogen_df = total_loading_df[case_river]\n",
    "    # included in SSM\n",
    "    total_wwtp_nitrogen_df = total_loading_df[all_wwtp]\n",
    "    total_river_nitrogen_df = total_loading_df[all_river]    \n",
    "    # total nitrogen loads, by source and scenario\n",
    "    wwtp_loads = total_loading_df[case_wwtp]\n",
    "    river_loads = total_loading_df[case_river]\n",
    "    # total nitrogen loads across local and all sources, by scenario\n",
    "    # rename columns in the process\n",
    "    total_wwtp_loading_local={}\n",
    "    total_river_loading_local={}\n",
    "    total_wwtp_loading_all={}\n",
    "    total_river_loading_all={}\n",
    "    for scenario in [*total_scenario_wwtp_nitrogen_df]:\n",
    "        total_wwtp_loading_local[ssm['run_information']['run_tag'][case][scenario]]=total_scenario_wwtp_nitrogen_df[scenario].sum()\n",
    "        total_river_loading_local[ssm['run_information']['run_tag'][case][scenario]]=total_scenario_river_nitrogen_df[scenario].sum()\n",
    "        total_wwtp_loading_all[ssm['run_information']['run_tag'][case][scenario]]=total_wwtp_nitrogen_df[scenario].sum()\n",
    "        total_river_loading_all[ssm['run_information']['run_tag'][case][scenario]]=total_river_nitrogen_df[scenario].sum()\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Creating dataframe with loading values\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    print('Creating WWTP and River loadings dataframe')\n",
    "    wwtp_load_df = pandas.DataFrame(wwtp_loads).sort_values(by=['wqm_baseline'], ascending=False)\n",
    "    river_load_df = pandas.DataFrame(river_loads).sort_values(by=['wqm_baseline'], ascending=False)\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Rename columns for report\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    wwtp_load_df=wwtp_load_df.rename(columns=ssm['run_information']['run_tag'][case])\n",
    "    river_load_df=river_load_df.rename(columns=ssm['run_information']['run_tag'][case])\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Creating dataframe with discharge values\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    wwtp_flow_df = pandas.DataFrame({'Annual Total Flow (m^3/year)':total_flow['wqm_baseline'][case_wwtp]}, index=ssm['WWTP_names'])\n",
    "    river_flow_df = pandas.DataFrame({'Annual Total Flow (m^3/year)':total_flow['wqm_baseline'][case_river]}, index=ssm['river_names'])\n",
    "    # combine river discharge and loading in locations where there are duplicates (R_1, R_2)\n",
    "    flow = []\n",
    "    river_load = {}\n",
    "    # River discharge (not scenario-dependant)\n",
    "    for river in ssm['unique_river_names']:      \n",
    "        flow.append(river_flow_df[river_flow_df.index.str.contains(river)][\"Annual Total Flow (m^3/year)\"].sum())\n",
    "    # River loading (scenario dependant)\n",
    "    for scenario in [*river_load_df]:\n",
    "        river_load[scenario]=[]\n",
    "        for river in ssm['unique_river_names']: \n",
    "            river_load[scenario].append(river_load_df[river_load_df.index.str.contains(river)][scenario].sum())\n",
    "    # dictionary of river discharge with one value per river instead of two (R_1 and R_2)\n",
    "    river_flow_df_corrected = pandas.DataFrame({'Annual Total Flow (m^3/year)':flow},index=ssm['unique_river_names'])\n",
    "    # dictionary of river loadings with one value per river instead of two (R_1 and R_2)\n",
    "    river_load_df_corrected = pandas.DataFrame(river_load,index=ssm['unique_river_names'])    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Combine dataframes with loading and discharge\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \n",
    "    # RIVERS\n",
    "    river_load_df_corrected[\"Annual Total Flow (m^3/year)\"] = river_flow_df_corrected[\"Annual Total Flow (m^3/year)\"]\n",
    "    # WWTP\n",
    "    wwtp_load_df[\"Annual Total Flow (m^3/year)\"] = wwtp_flow_df[\"Annual Total Flow (m^3/year)\"]\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Create dataframes for total loadings (a) used in this study, and (b) used in SSM\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \n",
    "    # RIVERS\n",
    "    total_river_loading_local_df = pandas.DataFrame(total_river_loading_local, index=['Total Rivers (altered in this report)'])\n",
    "    total_river_loading_all_df = pandas.DataFrame(total_river_loading_all, index=['Total Rivers (all in model domain) '])\n",
    "    # WWTP\n",
    "    total_wwtp_loading_local_df = pandas.DataFrame(total_wwtp_loading_local, index=['Total WWTPs (altered in this report)'])\n",
    "    total_wwtp_loading_all_df = pandas.DataFrame(total_wwtp_loading_all, index=['Total WWTPs (all in model domain) '])\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Combine dataframes with loadings/discharge with those showing total loadings in this study and all in SSM\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # RIVERS\n",
    "    river_load_df_corrected=river_load_df_corrected.append(total_river_loading_local_df)\n",
    "    river_load_df_corrected=river_load_df_corrected.append(total_river_loading_all_df)\n",
    "    # WWTP\n",
    "    wwtp_load_df=wwtp_load_df.append(total_wwtp_loading_local_df)\n",
    "    wwtp_load_df=wwtp_load_df.append(total_wwtp_loading_all_df)\n",
    "\n",
    "    \n",
    "   \n",
    "    # make README \n",
    "    this_file = '=HYPERLINK(\"https://github.com/RachaelDMueller/KingCounty-Rachael/blob/main/notebooks/reports/Table1_NutrientLoadings.ipynb\")'\n",
    "    run_description = '=HYPERLINK(\"https://github.com/RachaelDMueller/KingCounty-Rachael/blob/main/docs/supporting/KingCounty_Model_Runs.xlsx\",\"KingCounty_Model_Runs.xlsx (USING ORIGINAL RUN TAGS!!!)\")'\n",
    "\n",
    "    created_by = 'Rachael D. Mueller'\n",
    "    created_at = 'Puget Sound Institute'\n",
    "    created_from = 'Model results produced by Su Kyong Yun (PNNL) and Rachael Mueller (PSI)'\n",
    "    units='kg/year'\n",
    "    created_on = date.today().strftime(\"%B %d, %Y\")\n",
    "    header = {\n",
    "        ' ':[created_by, created_at, created_on, this_file, \n",
    "            units, created_from, \n",
    "            run_description]\n",
    "    }\n",
    "    header_df = pandas.DataFrame(header, index=[\n",
    "        'Created by',\n",
    "        'Created at',                           \n",
    "        'Created on',\n",
    "        'Created with',\n",
    "        'Loading units',\n",
    "        'Modeling by',\n",
    "        'Model Run Overview'])\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Save output to excel\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    excel_output_path = pathlib.Path('/mmfs1/gscratch/ssmc/USRS/PSI/Rachael/projects/KingCounty/reports')\n",
    "    print('*************************************************************')\n",
    "    print('Writing spreadsheet to: ',excel_output_path)\n",
    "    print('*************************************************************')\n",
    "    if os.path.exists(excel_output_path)==False:\n",
    "        print(f'creating: {excel_output_path}')\n",
    "        os.umask(0) #clears permissions\n",
    "        os.makedirs(excel_output_path, mode=0o777,exist_ok=True)\n",
    "    with pandas.ExcelWriter(\n",
    "        excel_output_path/f'Table1_NutrientLoadings_{case}.xlsx', mode='w') as writer:  \n",
    "        wwtp_load_df.to_excel(writer, sheet_name=f'WWTP ({case})')\n",
    "        river_load_df_corrected.to_excel(writer, sheet_name=f'Rivers ({case})')\n",
    "        header_df.to_excel(writer, sheet_name='README')\n",
    "    print(f'Number of WWTPs in this case: {len(total_scenario_wwtp_nitrogen_df)}')\n",
    "    print(f'Number of rivers in this case: {len(total_scenario_river_nitrogen_df)}')\n",
    "    print(f'Number of WWTPs in model: {len(total_wwtp_nitrogen_df)}')\n",
    "    print(f'Number of rivers in model: {len(total_river_nitrogen_df)}')\n",
    "    \n",
    "    return total_wwtp_nitrogen_df, total_river_nitrogen_df#wwtp_load_df, river_load_df_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ddfae01-f68a-42e9-8006-1885724333e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mmfs1/gscratch/ssmc/USRS/PSI/Rachael/projects/KingCounty/SalishSeaModel/4b/hotstart/inputs/ssm_pnt_wq.dat\n",
      "/mmfs1/gscratch/ssmc/USRS/PSI/Rachael/projects/KingCounty/SalishSeaModel/4c/hotstart/inputs/ssm_pnt_wq.dat\n",
      "/mmfs1/gscratch/ssmc/USRS/PSI/Sukyong/kingcounty/4x_inputs/ssm_pnt_wq_4c.dat\n",
      "/mmfs1/gscratch/ssmc/USRS/PSI/Sukyong/kingcounty/4x_inputs/ssm_pnt_wq_4d.dat\n",
      "/mmfs1/gscratch/ssmc/USRS/PSI/Sukyong/kingcounty/4x_inputs/ssm_pnt_wq_4e.dat\n",
      "/mmfs1/gscratch/ssmc/USRS/PSI/Sukyong/kingcounty/4x_inputs/ssm_pnt_wq_4f.dat\n",
      "/mmfs1/gscratch/ssmc/USRS/PSI/Sukyong/kingcounty/4x_inputs/ssm_pnt_wq_4g.dat\n",
      "/mmfs1/gscratch/ssmc/USRS/PSI/Sukyong/kingcounty/4x_inputs/ssm_pnt_wq_4h.dat\n",
      "/mmfs1/gscratch/ssmc/USRS/PSI/Sukyong/kingcounty/4x_inputs/ssm_pnt_wq_4i.dat\n",
      "/mmfs1/gscratch/ssmc/USRS/PSI/Sukyong/kingcounty/4x_inputs/ssm_pnt_wq_4j.dat\n",
      "/mmfs1/gscratch/ssmc/USRS/PSI/Rachael/projects/KingCounty/SalishSeaModel/4k/hotstart/inputs/ssm_pnt_wq.dat\n",
      "/mmfs1/gscratch/ssmc/USRS/PSI/Sukyong/kingcounty/4x_inputs/ssm_pnt_wq_4l.dat\n",
      "/mmfs1/gscratch/ssmc/USRS/PSI/Sukyong/kingcounty/WQM/WQM/hotstart/inputs/ssm_pnt_wq.dat\n",
      "/mmfs1/gscratch/ssmc/USRS/PSI/Sukyong/kingcounty/WQM_REF/WQM_REF/hotstart/inputs/ssm_pnt_wq.dat\n",
      "4b\n",
      "4c\n",
      "4csk\n",
      "4d\n",
      "4e\n",
      "4f\n",
      "4g\n",
      "4h\n",
      "4i\n",
      "4j\n",
      "4k\n",
      "4l\n",
      "wqm_baseline\n",
      "wqm_reference\n",
      "Creating WWTP and River loadings dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27856/3389395693.py:184: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  river_load_df_corrected=river_load_df_corrected.append(total_river_loading_local_df)\n",
      "/tmp/ipykernel_27856/3389395693.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  river_load_df_corrected=river_load_df_corrected.append(total_river_loading_all_df)\n",
      "/tmp/ipykernel_27856/3389395693.py:187: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  wwtp_load_df=wwtp_load_df.append(total_wwtp_loading_local_df)\n",
      "/tmp/ipykernel_27856/3389395693.py:188: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  wwtp_load_df=wwtp_load_df.append(total_wwtp_loading_all_df)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************************************\n",
      "Writing spreadsheet to:  /mmfs1/gscratch/ssmc/USRS/PSI/Rachael/projects/KingCounty/reports\n",
      "*************************************************************\n",
      "Number of WWTPs in this case: 3\n",
      "Number of rivers in this case: 2\n",
      "Number of WWTPs in model: 99\n",
      "Number of rivers in model: 160\n"
     ]
    }
   ],
   "source": [
    "wwtp_load_df, river_load_df = Table1_loadings('main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05facac8-d0ff-4eb5-9773-5765a7eada78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mmfs1/gscratch/ssmc/USRS/PSI/Sukyong/kingcounty/WQM/WQM/hotstart/inputs/ssm_pnt_wq.dat\n",
      "/mmfs1/gscratch/ssmc/USRS/PSI/Sukyong/kingcounty/WQM_REF/WQM_REF/hotstart/inputs/ssm_pnt_wq.dat\n",
      "/mmfs1/gscratch/ssmc/USRS/PSI/Sukyong/kingcounty/3b/inputs/ssm_pnt_wq.dat\n",
      "/mmfs1/gscratch/ssmc/USRS/PSI/Sukyong/kingcounty/3e/inputs/ssm_pnt_wq.dat\n",
      "/mmfs1/gscratch/ssmc/USRS/PSI/Sukyong/kingcounty/3f/inputs/ssm_pnt_wq.dat\n",
      "/mmfs1/gscratch/ssmc/USRS/PSI/Sukyong/kingcounty/3g/inputs/ssm_pnt_wq.dat\n",
      "/mmfs1/gscratch/ssmc/USRS/PSI/Sukyong/kingcounty/3h/inputs/ssm_pnt_wq.dat\n",
      "/mmfs1/gscratch/ssmc/USRS/PSI/Sukyong/kingcounty/3i/inputs/ssm_pnt_wq.dat\n",
      "/mmfs1/gscratch/ssmc/USRS/PSI/Rachael/projects/KingCounty/SalishSeaModel/3j/hotstart/inputs/ssm_pnt_wq.dat\n",
      "/mmfs1/gscratch/ssmc/USRS/PSI/Rachael/projects/KingCounty/SalishSeaModel/3k/hotstart/inputs/ssm_pnt_wq.dat\n",
      "/mmfs1/gscratch/ssmc/USRS/PSI/Sukyong/kingcounty/3c/inputs/ssm_pnt_wq.dat\n",
      "/mmfs1/gscratch/ssmc/USRS/PSI/Rachael/projects/KingCounty/SalishSeaModel/3l/hotstart/inputs/ssm_pnt_wq.dat\n",
      "/mmfs1/gscratch/ssmc/USRS/PSI/Sukyong/kingcounty/3m/inputs/ssm_pnt_wq.dat\n"
     ]
    }
   ],
   "source": [
    "case = 'whidbey'\n",
    "# open yaml configuration file created by notebook `SSM_config_{case}.ipynb` \n",
    "with open(f'../../etc/SSM_config_{case}.yaml', 'r') as file:\n",
    "    ssm = yaml.safe_load(file)\n",
    "# load list of directory paths for SSM nutrient loading .dat files\n",
    "runs = [*ssm['paths']['nutrient_loading_inputs']]\n",
    "# print out file list\n",
    "for run in runs:\n",
    "    print(ssm['paths']['nutrient_loading_inputs'][run])\n",
    "# establish directory path for ssm_pnt_wq_station_info.xlsx\n",
    "in_dir = pathlib.Path('/mmfs1/gscratch/ssmc/USRS/PSI/Rachael/projects/KingCounty/SalishSeaModel-spreadsheets')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ae7d18-bac9-4dee-bdfc-b07216e490fd",
   "metadata": {},
   "source": [
    "# Load 2014 loading values\n",
    "This information was saved to file while creating input files in `create_scenario_pnt_wq_3k.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe3aaa2-6707-42ef-85b3-191efb3135de",
   "metadata": {},
   "source": [
    "### Load loading inputs\n",
    "This code comes from Ben Roberts.  I got the same results with my method but like Ben's better because it covers all variables. See [ssm_read_pnt_wq.ipynb](https://github.com/bedaro/ssm-analysis/blob/main/input_files/ssm_read_pnt_wq.ipynb)\n",
    "\n",
    "From Ben Roberts:\n",
    "```\n",
    "The discharges are in m3/s, and cover a 24-hour period (technically the model linearly interpolates between them but that doesn't change the final result). The units for most constituents are mg/l and for the nitrogen constituents it's mg-N/l == g-N/m^3.\n",
    "\n",
    "So when I load the NetCDF into a dataset named 'ds' and do this:\n",
    "(ds['discharge'][:,0] * (ds['nh4'][:,0] + ds['no32'][:,0]) * 24 * 3600).sum()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e4e6c74-f030-4ca5-b7a5-83c1c6aacafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {}\n",
    "for run in runs:\n",
    "    with open(ssm['paths']['nutrient_loading_inputs'][run]) as f:\n",
    "        # The parsing logic here is is derived from the linkage instructions for the\n",
    "        # model and direct examination of the source code\n",
    "\n",
    "        # The first line does not contain important information and is treated only like\n",
    "        # a filetype magic\n",
    "        next(f)\n",
    "\n",
    "        # The total number of discharge nodes\n",
    "        num_qs = int(next(f))\n",
    "        # All the node numbers with discharges\n",
    "        #nodes = numpy.loadtxt([next(f) for l in range(num_qs)], comments='!', dtype=int)\n",
    "        node_raw = StringIO('\\n'.join([next(f) for l in range(num_qs)]))\n",
    "        node_df = pandas.read_csv(node_raw, sep='\\s+!\\s+', names=('Node','Comment'),\n",
    "                              dtype={'Node':numpy.int64,'Comment':object}, engine='python')\n",
    "        node_df.set_index('Node', inplace=True)\n",
    "        nodes = node_df.index.to_numpy()\n",
    "        # Depth distribution fractions into each node. Skipping the first (node count) column\n",
    "        vqdist = numpy.loadtxt([next(f) for l in range(num_qs)])[:,1:]\n",
    "\n",
    "        num_times = int(next(f))\n",
    "\n",
    "        # Initialize storage arrays\n",
    "        times = numpy.zeros(num_times)\n",
    "        qs = numpy.zeros((num_times, num_qs))\n",
    "        # State variables in the order they are present in the file. These are also going\n",
    "        # to be the NetCDF variable names\n",
    "        statevars = ('discharge', 'temp', 'salt', 'tss',  'alg1', 'alg2', 'alg3', 'zoo1',\n",
    "                                  'zoo2', 'ldoc', 'rdoc', 'lpoc', 'rpoc', 'nh4',  'no32',\n",
    "                                  'urea', 'ldon', 'rdon', 'lpon', 'rpon', 'po4',  'ldop',\n",
    "                                  'rdop', 'lpop', 'rpop', 'pip',  'cod',  'doxg', 'psi',\n",
    "                                  'dsi',  'alg1p','alg2p','alg3p','dic',  'talk')\n",
    "        inputs[run] = {}\n",
    "        for v in statevars:\n",
    "            inputs[run][v] = numpy.zeros((num_times, num_qs))\n",
    "\n",
    "        for t in range(num_times):\n",
    "            times[t] = float(next(f))\n",
    "            for v in statevars:\n",
    "                inputs[run][v][t,:] = numpy.loadtxt([next(f)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62536f01-5241-4c93-86c7-305b25f99271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating WWTP and River loadings dataframe\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'wwtp_loads' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Creating dataframe with loading values\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCreating WWTP and River loadings dataframe\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m wwtp_load_df \u001b[38;5;241m=\u001b[39m pandas\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mwwtp_loads\u001b[49m)\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwqm_baseline\u001b[39m\u001b[38;5;124m'\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m river_load_df \u001b[38;5;241m=\u001b[39m pandas\u001b[38;5;241m.\u001b[39mDataFrame(river_loads)\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwqm_baseline\u001b[39m\u001b[38;5;124m'\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Rename columns for report\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wwtp_loads' is not defined"
     ]
    }
   ],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Creating dataframe with loading values\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "print('Creating WWTP and River loadings dataframe')\n",
    "wwtp_load_df = pandas.DataFrame(wwtp_loads).sort_values(by=['wqm_baseline'], ascending=False)\n",
    "river_load_df = pandas.DataFrame(river_loads).sort_values(by=['wqm_baseline'], ascending=False)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Rename columns for report\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "wwtp_load_df=wwtp_load_df.rename(columns=ssm['run_information']['run_tag']['whidbey'])\n",
    "river_load_df=river_load_df.rename(columns=ssm['run_information']['run_tag']['whidbey'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b69adc0-9c09-4a4d-a3c1-67e5fd7188d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Creating dataframe with discharge values\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "wwtp_flow_df = pandas.DataFrame({'Annual Total Flow (m^3/year)':total_flow['wqm_baseline'][case_wwtp]}, index=ssm['WWTP_names'])\n",
    "river_flow_df = pandas.DataFrame({'Annual Total Flow (m^3/year)':total_flow['wqm_baseline'][case_river]}, index=ssm['river_names'])\n",
    "# combine river discharge and loading in locations where there are duplicates (R_1, R_2)\n",
    "flow = []\n",
    "river_load = {}\n",
    "# River discharge (not scenario-dependant)\n",
    "for river in ssm['unique_river_names']:      \n",
    "    flow.append(river_flow_df[river_flow_df.index.str.contains(river)][\"Annual Total Flow (m^3/year)\"].sum())\n",
    "# River loading (scenario dependant)\n",
    "for scenario in [*river_load_df]:\n",
    "    river_load[scenario]=[]\n",
    "    for river in ssm['unique_river_names']: \n",
    "        river_load[scenario].append(river_load_df[river_load_df.index.str.contains(river)][scenario].sum())\n",
    "# dictionary of river discharge with one value per river instead of two (R_1 and R_2)\n",
    "river_flow_df_corrected = pandas.DataFrame({'Annual Total Flow (m^3/year)':flow},index=ssm['unique_river_names'])\n",
    "# dictionary of river loadings with one value per river instead of two (R_1 and R_2)\n",
    "river_load_df_corrected = pandas.DataFrame(river_load,index=ssm['unique_river_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd99c54-eb13-4ce0-83e1-1b8caef478b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "river_flow_df_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b826497-ecab-46c8-9988-0ad19538f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined information of loading and discharge for each river\n",
    "river_load_df_corrected[\"Annual Total Flow (m^3/year)\"] = river_flow_df_corrected[\"Annual Total Flow (m^3/year)\"]\n",
    "# Add column with WWTP dischard to the loading dataframe\n",
    "wwtp_load_df[\"Annual Total Flow (m^3/year)\"] = wwtp_flow_df[\"Annual Total Flow (m^3/year)\"]\n",
    "# [WWTP] Create dataframes for total loadings by (a) WWTP used in this study, and (b) all WWTPs\n",
    "total_wwtp_loading_local_df = pandas.DataFrame(total_wwtp_loading_local, index=['Total WWTPs (altered in this report)'])\n",
    "total_wwtp_loading_all_df = pandas.DataFrame(total_wwtp_loading_all, index=['Total WWTPs (all in model domain) '])\n",
    "# [Rivers] Create dataframes for total loadings by (a) Rivers used in this study, and (b) all Rivers\n",
    "total_river_loading_local_df = pandas.DataFrame(total_river_loading_local, index=['Total Rivers (altered in this report)'])\n",
    "total_river_loading_all_df = pandas.DataFrame(total_river_loading_all, index=['Total Rivers (all in model domain) '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4b5283-e21f-4c52-a1c6-1a6615952bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Combine dataframes with loadings added across WWTP/rivers used in this study vs. all in SSM\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Add the rows with total loading for local wwtps/rivers and all wwtps/rivers\n",
    "wwtp_load_df=wwtp_load_df.append(total_wwtp_loading_local_df)\n",
    "wwtp_load_df=wwtp_load_df.append(total_wwtp_loading_all_df)\n",
    "\n",
    "river_load_df_corrected=river_load_df_corrected.append(total_river_loading_local_df)\n",
    "river_load_df_corrected=river_load_df_corrected.append(total_river_loading_all_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75da7f42-23f4-4743-afd1-db4373872db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "river_load_df_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3facf3d-6859-4882-8071-fedd5e7504d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make README \n",
    "this_file = '=HYPERLINK(\"https://github.com/RachaelDMueller/KingCounty-Rachael/blob/main/notebooks/reports/Table1_NutrientLoadings.ipynb\")'\n",
    "run_description = '=HYPERLINK(\"https://github.com/RachaelDMueller/KingCounty-Rachael/blob/main/docs/supporting/KingCounty_Model_Runs.xlsx\",\"KingCounty_Model_Runs.xlsx (USING ORIGINAL RUN TAGS!!!)\")'\n",
    "\n",
    "created_by = 'Rachael D. Mueller'\n",
    "created_at = 'Puget Sound Institute'\n",
    "created_from = 'Model results produced by Su Kyong Yun at the Salish Sea Modeling Center'\n",
    "units='kg/year'\n",
    "created_on = date.today().strftime(\"%B %d, %Y\")\n",
    "header = {\n",
    "    ' ':[created_by, created_at, created_on, this_file, \n",
    "        units, created_from, \n",
    "        run_description]\n",
    "}\n",
    "header_df = pandas.DataFrame(header, index=[\n",
    "    'Created by',\n",
    "    'Created at',                           \n",
    "    'Created on',\n",
    "    'Created with',\n",
    "    'Units',\n",
    "    'Modeling by',\n",
    "    'Model Run Overview'])\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Save output to excel\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "excel_output_path = pathlib.Path('/mmfs1/gscratch/ssmc/USRS/PSI/Rachael/projects/KingCounty/reports')\n",
    "print('*************************************************************')\n",
    "print('Writing spreadsheet to: ',excel_output_path)\n",
    "print('*************************************************************')\n",
    "if os.path.exists(excel_output_path)==False:\n",
    "    print(f'creating: {excel_output_path}')\n",
    "    os.umask(0) #clears permissions\n",
    "    os.makedirs(excel_output_path, mode=0o777,exist_ok=True)\n",
    "with pandas.ExcelWriter(\n",
    "    excel_output_path/f'Table1_NutrientLoadings_from_input_{case}.xlsx', mode='w') as writer:  \n",
    "    wwtp_load_df.to_excel(writer, sheet_name=f'WWTP ({case})')\n",
    "    river_load_df_corrected.to_excel(writer, sheet_name=f'Rivers ({case})')\n",
    "    header_df.to_excel(writer, sheet_name='README')\n",
    "print(f'Number of WWTPs in this case: {len(total_scenario_wwtp_nitrogen_df)}')\n",
    "print(f'Number of rivers in this case: {len(total_scenario_river_nitrogen_df)}')\n",
    "print(f'Number of WWTPs in model: {len(total_wwtp_nitrogen_df)}')\n",
    "print(f'Number of rivers in model: {len(total_river_nitrogen_df)}')\n",
    "\n",
    "\n",
    "# Test script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27dc240-8fe4-4571-bc94-6dffcb80c19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "case='whidbey'\n",
    "# open yaml configuration file created by notebook `SSM_config_{case}.ipynb` \n",
    "with open(f'../../etc/SSM_config_{case}.yaml', 'r') as file:\n",
    "    ssm = yaml.safe_load(file)\n",
    "# load list of directory paths for SSM nutrient loading .dat files\n",
    "runs = [*ssm['paths']['nutrient_loading_inputs']]\n",
    "# print out file list\n",
    "for run in runs:\n",
    "    print(ssm['paths']['nutrient_loading_inputs'][run])\n",
    "# # establish directory path for ssm_pnt_wq_station_info.xlsx\n",
    "# in_dir = pathlib.Path('/mmfs1/gscratch/ssmc/USRS/PSI/Rachael/projects/KingCounty/SalishSeaModel-spreadsheets')\n",
    "\n",
    "### START: move to util script ###\n",
    "\n",
    "# Adaptation of Ben's method for processing output    \n",
    "inputs = {}\n",
    "for run in runs:\n",
    "    with open(ssm['paths']['nutrient_loading_inputs'][run]) as f:\n",
    "        # The parsing logic here is is derived from the linkage instructions for the\n",
    "        # model and direct examination of the source code\n",
    "\n",
    "        # The first line does not contain important information and is treated only like\n",
    "        # a filetype magic\n",
    "        next(f)\n",
    "\n",
    "        # The total number of discharge nodes\n",
    "        num_qs = int(next(f))\n",
    "        # All the node numbers with discharges\n",
    "        #nodes = np.loadtxt([next(f) for l in range(num_qs)], comments='!', dtype=int)\n",
    "        node_raw = StringIO('\\n'.join([next(f) for l in range(num_qs)]))\n",
    "        node_df = pandas.read_csv(node_raw, sep='\\s+!\\s+', names=('Node','Comment'),\n",
    "                              dtype={'Node':numpy.int64,'Comment':object}, engine='python')\n",
    "        node_df.set_index('Node', inplace=True)\n",
    "        nodes = node_df.index.to_numpy()\n",
    "        # Depth distribution fractions into each node. Skipping the first (node count) column\n",
    "        vqdist = numpy.loadtxt([next(f) for l in range(num_qs)])[:,1:]\n",
    "\n",
    "        num_times = int(next(f))\n",
    "\n",
    "        # Initialize storage arrays\n",
    "        times = numpy.zeros(num_times)\n",
    "        qs = numpy.zeros((num_times, num_qs))\n",
    "        # State variables in the order they are present in the file. These are also going\n",
    "        # to be the NetCDF variable names\n",
    "        statevars = ('discharge', 'temp', 'salt', 'tss',  'alg1', 'alg2', 'alg3', 'zoo1',\n",
    "                                  'zoo2', 'ldoc', 'rdoc', 'lpoc', 'rpoc', 'nh4',  'no32',\n",
    "                                  'urea', 'ldon', 'rdon', 'lpon', 'rpon', 'po4',  'ldop',\n",
    "                                  'rdop', 'lpop', 'rpop', 'pip',  'cod',  'doxg', 'psi',\n",
    "                                  'dsi',  'alg1p','alg2p','alg3p','dic',  'talk')\n",
    "        inputs[run] = {}\n",
    "        for v in statevars:\n",
    "            inputs[run][v] = numpy.zeros((num_times, num_qs))\n",
    "\n",
    "        for t in range(num_times):\n",
    "            times[t] = float(next(f))\n",
    "            for v in statevars:\n",
    "                inputs[run][v][t,:] = numpy.loadtxt([next(f)])\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Create dictionaries of loading and discharge information by run\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "loading={}\n",
    "total_flow={}\n",
    "total_nitrogen={}\n",
    "#total_annual_nitrogen={}\n",
    "for run in runs:\n",
    "    print(run)\n",
    "    loading[run] = (inputs[run]['discharge'] * (inputs[run]['nh4'] + inputs[run]['no32']) * 24 * 3600)/1000 #m3/s*mg/l -> kg/day\n",
    "    total_flow[run] = (inputs[run]['discharge'] * 24 * 3600).sum(axis=0) # annual discharge m3/year\n",
    "    total_nitrogen[run]=loading[run].sum(axis=0) # annual loading over 366 days (g/year)\n",
    "    #total_annual_nitrogen[run] = total_nitrogen[run].sum() # g/year -> kg/year\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# import source location names from excel\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# information in excel spreadsheet: type (\" River\" or \" Point Source\"), node_id, layer_distribute, basin, country, NH4[kg/year], NO3NO2[kg/year], N-load\n",
    "source_locations = pandas.read_excel(in_dir/'ssm_pnt_wq_station_info.xlsx',index_col='Unnamed: 0')\n",
    "river_locations = source_locations[source_locations['type']==' River']\n",
    "wwtp_locations = source_locations[source_locations['type']==' Point Source']\n",
    "# make lists of names for: \n",
    "all_rivers_list = river_locations.index.to_list() # river input locations\n",
    "all_wwtps_list = wwtp_locations.index.to_list()   # WWTP input locations\n",
    "all_locations_list = source_locations.index.to_list()  # both river and WWTP input locations\n",
    "all_locations_type_list = source_locations['type'].to_list() # type of input (\" River\" or \" Point Source\") to eliminate duplicate names\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Create dataframes of information using names from excel as indices\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# NOTE: Some names are redundant for rivers and WWTP so we need to use both name and type (\" River\" vs. \" Point Source\")\n",
    "total_flow_df  = pandas.DataFrame(\n",
    "    total_flow, \n",
    "    index=all_locations_list,\n",
    "    columns=ssm['run_information']['run_tag']['whidbey'] # Assign report names to columns\n",
    ")\n",
    "total_loading_df = pandas.DataFrame(\n",
    "    total_nitrogen, \n",
    "    index=all_locations_list,\n",
    "    columns=ssm['run_information']['run_tag']['whidbey'] # Assign report names to columns\n",
    ")\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Use both name of source and type to create lists of wwtp and river sources\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Create boolean arrays with True for only WWTP and River sources used in this cases \n",
    "case_wwtp = numpy.asarray(total_loading_df.index.isin(ssm['WWTP_names']) & (source_locations['type']==' Point Source'))\n",
    "case_river = numpy.asarray(total_loading_df.index.isin(ssm['river_names']) & (source_locations['type']==' River'))\n",
    "# Create boolean arrays with True for all WWTP and River sources used in this study\n",
    "all_wwtp= numpy.asarray(total_loading_df.index.isin(all_wwtps_list) & (source_locations['type']==' Point Source'))\n",
    "all_river = numpy.asarray(total_loading_df.index.isin(all_rivers_list) & (source_locations['type']==' River'))\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Calculate the total nitrogen loading for rivers and wwtps\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# modified in a particular case\n",
    "total_scenario_wwtp_nitrogen_df = total_loading_df[case_wwtp]\n",
    "total_scenario_river_nitrogen_df = total_loading_df[case_river]\n",
    "# included in SSM\n",
    "total_wwtp_nitrogen_df = total_loading_df[all_wwtp]\n",
    "total_river_nitrogen_df = total_loading_df[all_river]\n",
    "# total nitrogen loads, by source and scenario\n",
    "wwtp_loads = total_loading_df[case_wwtp]\n",
    "river_loads = total_loading_df[case_river]\n",
    "# total nitrogen loads across local and all sources, by scenario\n",
    "# rename columns in the process\n",
    "total_wwtp_loading_local={}\n",
    "total_river_loading_local={}\n",
    "total_wwtp_loading_all={}\n",
    "total_river_loading_all={}\n",
    "for scenario in [*total_scenario_wwtp_nitrogen_df]:\n",
    "    total_wwtp_loading_local[ssm['run_information']['run_tag']['whidbey'][scenario]]=total_scenario_wwtp_nitrogen_df[scenario].sum()\n",
    "    total_river_loading_local[ssm['run_information']['run_tag']['whidbey'][scenario]]=total_scenario_river_nitrogen_df[scenario].sum()\n",
    "    total_wwtp_loading_all[ssm['run_information']['run_tag']['whidbey'][scenario]]=total_wwtp_nitrogen_df[scenario].sum()\n",
    "    total_river_loading_all[ssm['run_information']['run_tag']['whidbey'][scenario]]=total_river_nitrogen_df[scenario].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c382b-f4d5-40bd-83b2-d1f7fbf4e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wwtp_load_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45fce34-cf78-4760-a0d5-55e3ad70cae5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
